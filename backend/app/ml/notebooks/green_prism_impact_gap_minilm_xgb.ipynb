{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Green Prism \u2014 Impact Realization Gap Model (MiniLM + XGBoost)\n", "\n", "This notebook trains an **Impact Realization Gap** model.\n", "\n", "**Goal:** predict **actual environmental impact** (e.g. tons CO\u2082 avoided) from:\n", "\n", "- Bond / project **disclosure text** (or a structured summary)\n", "- **Claimed** impact (`claimed_impact_co2_tons`)\n", "- Bond / project **metadata**:\n", "  - sector, region, technology (solar/wind/transport/etc.)\n", "  - use-of-proceeds category\n", "  - bond size, tenor\n", "\n", "We then compute for each record:\n", "\n", "- **Realization ratio**: `ratio = actual / claimed`\n", "- **Impact gap**: `gap = claimed - predicted_actual`\n", "\n", "---\n", "\n", "## 0. Prerequisites\n", "\n", "Before running this notebook, create a unified training CSV at:\n", "\n", "`backend/app/data/impact_training_data.csv`\n", "\n", "with at least the following columns:\n", "\n", "- `text` \u2014 disclosure or impact report text (or a stitched summary from ADB / WB tables)\n", "- `claimed_impact_co2_tons` \u2014 claimed CO\u2082 impact (tons)\n", "- `actual_impact_co2_tons` \u2014 observed / realized CO\u2082 impact (tons)\n", "\n", "Optional (recommended) metadata columns:\n", "\n", "- `sector` \u2014 project or issuer sector (e.g. 'Energy', 'Transport')\n", "- `region` \u2014 region / country group (e.g. 'EMEA', 'Asia', 'Pacific')\n", "- `technology` \u2014 coarse tech bucket (e.g. 'solar', 'wind', 'metro', 'hydro')\n", "- `use_of_proceeds` \u2014 UoP category (e.g. 'Renewable Energy', 'Clean Transport')\n", "- `amount_issued_usd` \u2014 bond size in USD\n", "- `tenor_years` \u2014 bond tenor in years\n", "\n", "You can populate this CSV by extracting rows from:\n", "\n", "- **ADB Green & Blue Bond Impact Report (Excel)** \u2014 projects with claimed + realized impacts\n", "- **World Bank / IFC** impact reports \u2014 wherever both claimed & realized metrics are published\n", "\n", "The cells below assume that file exists and use it for training.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Imports & Path Setup"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from pathlib import Path\n", "import sys\n", "\n", "import numpy as np\n", "import pandas as pd\n", "\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import mean_squared_error, r2_score\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "from xgboost import XGBRegressor\n", "\n", "import torch\n", "from sentence_transformers import SentenceTransformer\n", "\n", "# Repo layout assumptions:\n", "#   repo_root/\n", "#     backend/\n", "#       app/\n", "#         data/impact_training_data.csv\n", "#\n", "REPO_ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name == \"notebooks\" else Path.cwd().resolve()\n", "BACKEND_ROOT = REPO_ROOT / \"backend\"\n", "DATA_PATH = BACKEND_ROOT / \"app\" / \"data\" / \"impact_training_data.csv\"\n", "\n", "print(\"CWD:\", Path.cwd())\n", "print(\"REPO_ROOT:\", REPO_ROOT)\n", "print(\"BACKEND_ROOT exists:\", BACKEND_ROOT.exists())\n", "print(\"DATA_PATH:\", DATA_PATH)\n", "\n", "sys.path.insert(0, str(BACKEND_ROOT))\n", "\n", "from app.ml.preprocessing import clean_text\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Load & Inspect Training Data"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["if not DATA_PATH.exists():\n", "    raise FileNotFoundError(\n", "        f\"Training CSV not found at {DATA_PATH}.\\n\"\n", "        \"Create impact_training_data.csv with at least the columns \"\n", "        \"[text, claimed_impact_co2_tons, actual_impact_co2_tons].\"\n", "    )\n", "\n", "df = pd.read_csv(DATA_PATH)\n", "print(\"Raw shape:\", df.shape)\n", "df.head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1 Basic Cleaning & Column Normalization"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Rename columns if your CSV uses slightly different names.\n", "# Adjust the mapping below to match your file.\n", "col_map = {\n", "    \"text\": \"text\",\n", "    \"claimed_impact_co2_tons\": \"claimed_impact_co2_tons\",\n", "    \"actual_impact_co2_tons\": \"actual_impact_co2_tons\",\n", "    \"sector\": \"sector\",\n", "    \"region\": \"region\",\n", "    \"technology\": \"technology\",\n", "    \"use_of_proceeds\": \"use_of_proceeds\",\n", "    \"amount_issued_usd\": \"amount_issued_usd\",\n", "    \"tenor_years\": \"tenor_years\",\n", "}\n", "\n", "df = df.rename(columns=col_map)\n", "\n", "required_cols = [\"text\", \"claimed_impact_co2_tons\", \"actual_impact_co2_tons\"]\n", "missing = [c for c in required_cols if c not in df.columns]\n", "if missing:\n", "    raise ValueError(f\"Missing required columns in CSV: {missing}\")\n", "\n", "# Drop rows with missing claimed/actual or text\n", "df = df.dropna(subset=[\"text\", \"claimed_impact_co2_tons\", \"actual_impact_co2_tons\"]).copy()\n", "df[\"text\"] = df[\"text\"].astype(str)\n", "\n", "# Ensure numeric\n", "for c in [\"claimed_impact_co2_tons\", \"actual_impact_co2_tons\",\n", "          \"amount_issued_usd\", \"tenor_years\"]:\n", "    if c in df.columns:\n", "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n", "\n", "df = df.dropna(subset=[\"claimed_impact_co2_tons\", \"actual_impact_co2_tons\"]).copy()\n", "\n", "print(\"After cleaning:\", df.shape)\n", "df[[\"claimed_impact_co2_tons\", \"actual_impact_co2_tons\"]].describe()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 Realization Ratio & Gap (for inspection only)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["df[\"realization_ratio\"] = df[\"actual_impact_co2_tons\"] / df[\"claimed_impact_co2_tons\"].replace(0, np.nan)\n", "df[\"impact_gap\"] = df[\"claimed_impact_co2_tons\"] - df[\"actual_impact_co2_tons\"]\n", "\n", "df[[\"claimed_impact_co2_tons\", \"actual_impact_co2_tons\", \"realization_ratio\", \"impact_gap\"]].head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Clean Text"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["df[\"clean_text\"] = df[\"text\"].fillna(\"\").astype(str).apply(clean_text)\n", "df[[\"clean_text\"]].head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Technology Buckets (Solar / Wind / Transport / etc.)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# We create a coarse 'tech_bucket' feature based on free-text 'technology' or 'use_of_proceeds'.\n", "\n", "def infer_tech_bucket(row):\n", "    text = (str(row.get(\"technology\", \"\")) + \" \" + str(row.get(\"use_of_proceeds\", \"\"))).lower()\n", "    if any(k in text for k in [\"solar\", \"pv\", \"photovoltaic\"]):\n", "        return \"solar\"\n", "    if any(k in text for k in [\"wind\", \"offshore wind\", \"onshore wind\"]):\n", "        return \"wind\"\n", "    if any(k in text for k in [\"metro\", \"rail\", \"bus\", \"transport\", \"subway\", \"tram\"]):\n", "        return \"transport\"\n", "    if any(k in text for k in [\"hydro\", \"geothermal\"]):\n", "        return \"hydro_geo\"\n", "    if any(k in text for k in [\"building\", \"efficiency\", \"retrofit\"]):\n", "        return \"buildings_efficiency\"\n", "    if any(k in text for k in [\"water\", \"wastewater\", \"sewer\", \"drinking water\"]):\n", "        return \"water\"\n", "    if any(k in text for k in [\"waste\", \"landfill\", \"recycling\"]):\n", "        return \"waste\"\n", "    return \"other\"\n", "\n", "df[\"tech_bucket\"] = df.apply(infer_tech_bucket, axis=1)\n", "df[[\"technology\", \"use_of_proceeds\", \"tech_bucket\"]].head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Categorical Encodings & Numeric Features"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Define metadata columns we will use (if present)\n", "cat_cols = [c for c in [\"sector\", \"region\", \"technology\", \"use_of_proceeds\", \"tech_bucket\"] if c in df.columns]\n", "num_cols = [c for c in [\"claimed_impact_co2_tons\", \"amount_issued_usd\", \"tenor_years\"] if c in df.columns]\n", "\n", "print(\"Categorical columns:\", cat_cols)\n", "print(\"Numeric columns:\", num_cols)\n", "\n", "# Simple label-encoding for categoricals\n", "cat_maps = {}\n", "for col in cat_cols:\n", "    df[col] = df[col].fillna(\"UNKNOWN\").astype(str)\n", "    uniques = sorted(df[col].unique())\n", "    mapping = {val: i for i, val in enumerate(uniques)}\n", "    cat_maps[col] = mapping\n", "    df[col + \"_idx\"] = df[col].map(mapping)\n", "\n", "# Build numeric feature array\n", "num_features = []\n", "for col in num_cols:\n", "    vals = df[col].fillna(0.0).astype(float).values.reshape(-1, 1)\n", "    num_features.append(vals)\n", "\n", "if num_features:\n", "    num_features = np.concatenate(num_features, axis=1)\n", "else:\n", "    num_features = np.zeros((len(df), 0), dtype=np.float32)\n", "\n", "# Add categorical indices as numeric features\n", "for col in cat_cols:\n", "    idx_vals = df[col + \"_idx\"].astype(float).values.reshape(-1, 1)\n", "    num_features = np.concatenate([num_features, idx_vals], axis=1)\n", "\n", "print(\"Numeric+cat feature shape:\", num_features.shape)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Text Embeddings (MiniLM Sentence Transformer)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n", "\n", "st_model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n", "print(\"Loaded sentence-transformers model:\", MODEL_NAME, \"on\", DEVICE)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["texts_clean = df[\"clean_text\"].tolist()\n", "\n", "print(\"Embedding texts with MiniLM...\")\n", "text_embeddings = st_model.encode(texts_clean, batch_size=32, show_progress_bar=True)\n", "text_embeddings = np.asarray(text_embeddings, dtype=np.float32)\n", "print(\"Embeddings shape:\", text_embeddings.shape)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 6.1 Build Final Feature Matrix X and Target y"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Combine text embeddings + numeric metadata\n", "X = np.concatenate([text_embeddings, num_features], axis=1)\n", "\n", "# Target: actual impact (tons CO2)\n", "y = df[\"actual_impact_co2_tons\"].astype(float).values\n", "\n", "print(\"X shape:\", X.shape, \"  y shape:\", y.shape)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Train XGBoost Regressor (Actual Impact)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["X_train, X_val, y_train, y_val = train_test_split(\n", "    X, y, test_size=0.2, random_state=42\n", ")\n", "\n", "# Optional: scale targets if very skewed; here we keep raw tons CO2 for interpretability.\n", "\n", "# A relatively small XGBoost model as MVP\n", "xgb = XGBRegressor(\n", "    n_estimators=400,\n", "    learning_rate=0.05,\n", "    max_depth=4,\n", "    subsample=0.9,\n", "    colsample_bytree=0.9,\n", "    objective=\"reg:squarederror\",\n", "    random_state=42,\n", "    n_jobs=4,\n", ")\n", "\n", "xgb.fit(X_train, y_train)\n", "y_pred = xgb.predict(X_val)\n", "\n", "rmse = mean_squared_error(y_val, y_pred, squared=False)\n", "r2 = r2_score(y_val, y_pred)\n", "\n", "print(f\"Validation RMSE: {rmse:.3f} tons CO2\")\n", "print(f\"Validation R^2: {r2:.3f}\")\n", "\n", "print(\"Actual stats: mean\", y_val.mean(), \"std\", y_val.std())\n", "print(\"Pred stats:   mean\", y_pred.mean(), \"std\", y_pred.std())\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 7.1 Realization Ratio & Gap on Validation Set"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["val_claimed = df.loc[y_val.index if hasattr(y_val, 'index') else X_val.shape[0]:, \"claimed_impact_co2_tons\"] if False else None"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# To compute ratio/gap metrics on the validation set,\n", "# we need the subset of 'claimed' values aligned with X_val/y_val.\n", "\n", "# Re-do split on claimed column with same random_state & test_size\n", "claimed = df[\"claimed_impact_co2_tons\"].astype(float).values\n", "_, claimed_val = train_test_split(\n", "    claimed, test_size=0.2, random_state=42\n", ")\n", "\n", "ratio_val = claimed_val / np.where(y_pred == 0, np.nan, y_pred)\n", "gap_val = claimed_val - y_pred\n", "\n", "print(\"Realization ratio (claimed / pred_actual) \u2014 summary:\")\n", "print(pd.Series(ratio_val).describe())\n", "\n", "print(\"\\nImpact gap (claimed - pred_actual) \u2014 summary:\")\n", "print(pd.Series(gap_val).describe())\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 7.2 Diagnostic Plot"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "\n", "plt.figure(figsize=(5, 5))\n", "plt.scatter(y_val, y_pred, alpha=0.4)\n", "plt.xlabel(\"Actual CO2 impact (tons)\")\n", "plt.ylabel(\"Predicted CO2 impact (tons)\")\n", "plt.title(\"Actual vs Predicted Impact (XGBoost + MiniLM)\")\n", "min_val = float(min(y_val.min(), y_pred.min()))\n", "max_val = float(max(y_val.max(), y_pred.max()))\n", "plt.plot([min_val, max_val], [min_val, max_val], \"r--\", alpha=0.6)\n", "plt.grid(True)\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Save Model Artifact"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from joblib import dump\n", "\n", "MODEL_DIR = BACKEND_ROOT / \"app\" / \"models\"\n", "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n", "MODEL_PATH = MODEL_DIR / \"impact_gap_xgb_minilm.joblib\"\n", "\n", "artifact = {\n", "    \"model\": xgb,\n", "    \"text_model_name\": MODEL_NAME,\n", "    \"cat_maps\": cat_maps,\n", "    \"num_cols\": num_cols,\n", "    \"cat_cols\": cat_cols,\n", "}\n", "\n", "dump(artifact, MODEL_PATH)\n", "print(\"Saved impact gap model to:\", MODEL_PATH)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Inference Helper \u2014 Predict Impact for One Bond/Project"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from joblib import load\n", "\n", "def load_impact_model(model_path: Path = MODEL_PATH):\n", "    return load(model_path)\n", "\n", "def _encode_metadata(row: dict, artifact: dict) -> np.ndarray:\n", "    num_cols = artifact[\"num_cols\"]\n", "    cat_cols = artifact[\"cat_cols\"]\n", "    cat_maps = artifact[\"cat_maps\"]\n", "\n", "    feats = []\n", "\n", "    # numeric\n", "    for col in num_cols:\n", "        val = row.get(col, 0.0)\n", "        try:\n", "            v = float(val)\n", "        except Exception:\n", "            v = 0.0\n", "        feats.append(v)\n", "\n", "    # categorical as label indices\n", "    for col in cat_cols:\n", "        mapping = cat_maps[col]\n", "        raw_val = str(row.get(col, \"UNKNOWN\"))\n", "        idx = mapping.get(raw_val, mapping.get(\"UNKNOWN\", 0))\n", "        feats.append(float(idx))\n", "\n", "    return np.array(feats, dtype=np.float32).reshape(1, -1)\n", "\n", "@torch.no_grad()\n", "def predict_impact_gap_ml(\n", "    text: str,\n", "    claimed_impact_co2_tons: float,\n", "    meta: dict,\n", "    artifact: dict,\n", "    st_model_for_inference: SentenceTransformer,\n", "):\n", "    \"\"\"\n", "    Returns:\n", "      {\n", "        'predicted_impact_mean': float,\n", "        'predicted_impact_std': float (placeholder, can refine later),\n", "        'gap': claimed - predicted_mean,\n", "        'realization_ratio': predicted_mean / claimed (if claimed>0)\n", "      }\n", "    \"\"\"\n", "    # text embedding\n", "    cleaned = clean_text(text)\n", "    emb = st_model_for_inference.encode([cleaned])\n", "    emb = np.asarray(emb, dtype=np.float32)  # (1, H)\n", "\n", "    # ensure claimed impact is in meta for numeric features if used\n", "    meta = dict(meta)\n", "    meta.setdefault(\"claimed_impact_co2_tons\", claimed_impact_co2_tons)\n", "\n", "    meta_vec = _encode_metadata(meta, artifact)  # (1, M)\n", "\n", "    feats = np.concatenate([emb, meta_vec], axis=1)  # (1, H+M)\n", "\n", "    model = artifact[\"model\"]\n", "    pred = float(model.predict(feats)[0])\n", "\n", "    # Placeholder uncertainty: we can plug in a more advanced method later\n", "    predicted_std = float(abs(pred) * 0.15)  # e.g. 15% of magnitude\n", "\n", "    gap = float(claimed_impact_co2_tons - pred)\n", "    realization_ratio = float(pred / claimed_impact_co2_tons) if claimed_impact_co2_tons > 0 else None\n", "\n", "    return {\n", "      \"predicted_impact_mean\": pred,\n", "      \"predicted_impact_std\": predicted_std,\n", "      \"gap\": gap,\n", "      \"realization_ratio\": realization_ratio,\n", "    }\n", "\n", "artifact_loaded = load_impact_model()\n", "\n", "# Example usage on the first row\n", "example_row = df.iloc[0]\n", "example_meta = {col: example_row.get(col) for col in (cat_cols + num_cols)}\n", "\n", "example_result = predict_impact_gap_ml(\n", "    text=example_row[\"text\"],\n", "    claimed_impact_co2_tons=float(example_row[\"claimed_impact_co2_tons\"]),\n", "    meta=example_meta,\n", "    artifact=artifact_loaded,\n", "    st_model_for_inference=st_model,\n", ")\n", "\n", "example_row[[\"claimed_impact_co2_tons\", \"actual_impact_co2_tons\"]], example_result\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "## 10. Next Steps\n", "\n", "- Move `predict_impact_gap_ml` into a backend module, e.g.\n", "  `app/ml/impact_gap_model_ml.py`.\n", "- In your FastAPI service, call this from the scoring service instead of (or in\n", "  addition to) the current placeholder impact model.\n", "- In the React UI, display:\n", "  - `claimed` vs `predicted_impact_mean`\n", "  - `gap = claimed - predicted`\n", "  - An uncertainty band: `predicted \u00b1 predicted_impact_std`\n", "  - A qualitative label, e.g. `Aligned / Overstated / Understated` based on the gap.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}